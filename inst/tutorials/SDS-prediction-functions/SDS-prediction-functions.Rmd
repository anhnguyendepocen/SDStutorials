---
title: "Building and evaluating prediction functions"
subtitle: "Stats for Data Science"
author: "D. Kaplan"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(SDSdata)
library(mosaic)
library(mosaicModel)
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

A typical process of making a statistical prediction involves several steps:

1. Collecting data.
2. Choosing a response and one or more explanatory variables.
3. Choosing a model architecture.
4. Training the model on the data so that the model replicates the patterns seen in the data.
5. Evaluating the model. That is, providing values for the explanatory variables as inputs and reading off the corresponding output.

This tutorial introduces computing techniques for accomplishing steps (4) and (5). We'll  take it for granted that you already have (1) data and have decided (2) which variable to use as the response and which variable(s) will be explanatory. As regards  (3) choosing a model architecture, this tutorial will focus on two different architectures: linear and logistic. 

## Linear models

The linear architecture is appropriate when the response variable is *quantitative*. It doesn't matter whether the explanatory variables are quantitative or categorical; the linear architecture is suitable for all kinds of explanatory variables.

Training a linear model is accomplished with the `lm()` function  in  R. (`lm` stands for `l`inear `m`odel.) You need to provide two inputs to `lm()`:

a. a data frame to use for training
b. an model formula specifying the explanatory variable(s) and the response variable.

The output of `lm()` is a trained model. The output is usually given a name so that you can refer to it later. You'll  need to do this in order to *evaluate* the model on new explanatory inputs in order to use the model to make a prediction about the value of the response variable that corresponds to those inputs.

To illustrate, you will build a predictive model of the amount of natural gas used for heating a home. The data come from the monthly utility bills for the author's home in Minnesota, USA. The data frame is named `Home_utilities` and comes from the `SDSdata` package. 

The response variable -- the amount of natural gas actually used -- will be `ccf`, which  stands for the volume of gas used in  one month.  (The volume units for natural gas are cubic feet, appreviated ccf.). To start, we'll try to predict `ccf` based on the average temperature, `temp`, during the month.

As you can see in the following code block, the  `lm()` function  is being used. The two arguments to  `lm()` are the model formula and the data  frame. As always, the reponse variable is on the left  side  of the formula. There's only one explanatory variable being used here. It goes on the right side of the formula. The output of `lm()` has been saved under the name `mod1`.

```{r ccf-temp1, exercise = TRUE, exercise.cap = "Train the model"}
mod1 <- lm(ccf ~ temp, data = Home_utilities)
```

Training is very simple; all the work is done by `lm()`. Once you have the trained model, you  will want to do something with it. You have some choices:

1. Plot a graph of the model. Use `mod_plot(mod1)`.
2. Evaluate the model by specifying an input. For instance, to see the predicted natural gas usage when the monthly average temperature is 32°F (corresponding to 0°C) use `mod_eval(mod1, temp = 32)`. 

```{r ccf-temp2, echo = FALSE}
Q1 <- question("Make a graph of the `ccf ~ temp` model. What is the shape of the graph?",
               answer("A constant."),
               answer("A straight line.", correct = TRUE),
               answer("A wavy curve."))
Q2 <- question("Does the output of the model go up or down was the temperature increases? You can judge this from the slope of the graph.", 
               answer("Down. Higher temperature goes with lower `ccf`.", correct = TRUE),
               answer("Up. Higher temperature goes with higher `ccf`.  "),
               answer("Both up and  down."))
Q3 <- question("What is the numerical output of the model when the input is `temp  = 20`?",
               answer("186 cubic feet"),
               answer("187 cubic feet"),
               answer("188 cubic feet", correct = TRUE))
quiz(caption="Looking at `ccf` versus `temp`.", Q1, Q2, Q3)
```

## Prediction intervals

A prediction ought to give an indication of the probability of each  possible outcome.  As an approximation to this, you can ask `mod_eval()` to generate a 95% prediction interval. Do this by  giving `mod_eval()` another argument: `interval = "prediction"`.

The `mod_plot()` function can also take the `interval = "prediction"` argument.

```{r ccf-temp3, exercise = TRUE, exercise.cap = "With prediction intervals"}
mod1 <- lm(ccf ~ temp, data = Home_utilities)
mod_eval(mod1, temp = 32,  interval = "prediction")
mod_plot(mod1, interval = "prediction")
```

```{r ccf-temp4, echo = FALSE}
Q1 <- question("What is the 95% prediction interval for `ccf` when `temp = 40`?",
               answer("97 to  194 ccf", message="Sorry. That's the prediction interval for `temp = 32`. You were asked about  `temp  = 40`."),
               answer("69 to 165 ccf", correct = TRUE),
               answer("69 to 194 ccf."))
Q2 <- question("In the graph of the model, how is the prediction interval shown?", 
               answer("Error bars"),
               answer("A band shaded gray.", correct = TRUE),
               answer("Dotted lines."))

quiz(caption="Prediction intervals", Q1, Q2)
```

## Not-straight models with `ns()`

Even though the model architecture is called "linear," graphs of the model do not need to be straight lines. In order to have a curvy function,  you will need to say so in the model formula used for training the model. You can do this by giving the explanatory variable as an argument to  the `ns()` function. `ns()` also takes a second argument, called the "degrees of freedom", which can be 1, 2, 3, ... The degrees of freedom describes how much curviness to allow.

```{r ccf-temp5, exercise = TRUE, exercise.cap = "Curvy models with `ns()`."}
mod2 <- lm(ccf ~ ns(temp, 0), data = Home_utilities)
mod_plot(mod2, interval  = "prediction")
```

It's nice to imagine that `ns` means `n`ot `s`traight. It does effectively, but the stickler for mathematical precision will point out that it actually stands for `n`atural `s`pline. A spline is a somewhat stiff but somewhat flexible curve.

```{r ccf-temp6, echo = FALSE}
Q1 <- question("The second argument to `ns()` can be 1, 2, 3,  .... What value for the degrees of freedom corresponds to a straight line.",
               answer("1", correct = TRUE),
               answer("2"), 
               answer("3"))
Q2 <- question("If you make the degrees of freedom large, the model can go up and down somewhat erratically. Which  of these degrees of freedom  produces lots of little ups and downs? ", 
               answer("5"),
               answer("10"),
               answer("20", correct  = TRUE))
quiz(caption="Nonlinear curves", Q1, Q2)
```

## Logistic models

The logistic architecture is appropriate when the response variable is categorical and you want to find the probability that the output will be a particular one of the possible levels. Training a logistic model is done with the `glm()` function. Like `lm()`, you provide a model formula and a data frame as arguments. You must also provide a third argument, which will always be `family = binomial`.

The explanatory variable(s) in a logistic model can be quantitative or categorical

To illustrate, let's model the sex of a person using their height as the explanatory variable. The `Galton` data frame records the `sex` and `height` of about 900 adults measured in the 1890s.

When training a logistic model, you need to write the model formula so that the left-hand side specifies which level you are asking the probability for. This is done by naming the level in quotes and using `==` (double equal signs), which stands for "is the thing on the left equal to the thing on the right?" In `Galton`, the levels for `sex` are `"F"` and `"M"`.

```{r logistic-1, exercise = TRUE, exercise.cap = "Logistic model"}
sex_mod1 <- glm(sex == "F" ~ height, data = Galton, family = binomial)
mod_plot(sex_mod1)
mod_eval(sex_mod1, height = 65)
```

Note that the shape of the logistic model bends as needed to keep the output between 0 and 1, as befits the output's being a probability.

```{r logistic-2, echo = FALSE}
Q1 <- question("When `height = 70` inches, what is the probability of the person being female?",
               answer("zero"),
               answer("About 6%", correct = TRUE),
               answer("About 30%"))
Q2 <- question("When `height = 65` inches, what is the  probability  of the person being male?", 
               answer("About 23%", correct = TRUE),
               answer("About 50%"),
               answer("About 77%", message = "That's the probability of being female. You were asked about the probability of being male."))
Q3 <- question("What happens if you leave out the argument `family = binomial`?",
               answer("R figures out what you meant."),
               answer("R generates an error message."),
               answer("R makes a straight-line model (that doesn't stay in the bounds zero to one).", correct = TRUE))
Q4 <- question("What happens if instead of using two equal signs (`==`) in the formula, you use just one equal sign (`=`)?",
                              answer("R figures out what you meant."),
               answer("R generates an error message.", correct = TRUE),
               answer("R makes a straight-line model (that doesn't stay in the bounds zero to one)."))
quiz(caption = "Logistic regression", Q1, Q2, Q3, Q4)               

```

## Multiple explanatory variables

You can include multiple explanatory variables by adding their names to the model formula with `+`.  Example: Predict the sex of a person based on the person's height and also the person's mother's height.

```{r multiple-1, exercise = TRUE, exercise.cap = "Two explanatory variables"}
sex_mod2 <- glm(sex == "F" ~ height + mother, data = Galton, family = binomial)
mod_plot(sex_mod2)
mod_eval(sex_mod2,  height = 65,  mother = 61)
```

```{r multiple-2, echo = FALSE}
Q1 <- question("In the plot of the model made by `mod_plot()`,  how is the second explanatory variable displayed?",
               answer("With color", correct = TRUE),
               answer("With facetting"),
               answer("It isn't displayed."))
Q2 <- question("For a person of height 65 inches, is the person more  likely to be female if the mother's height is 62 or 63 inches?",
               answer("Higher probability of being female when mother is 62 inches."), 
               answer("Higher probability of being female when mother is 63 inches.", correct = TRUE),
               answer("The same probability regardless of whether the mother is 62 or 63 inches tall.", message = "Make sure you are looking at the output of `sex_mod2` rather than `sex_mod1`. Remember that `sex_mod1` doesn't include the mother's height as an explanatory variable."))
Q3 <- question("Include the father's height as an explanatory variable by using the model formula `sex == \"F\" ~ height + mother + father`. In the graph of the model function, how is `father` displayed?",
               answer("With color"),
               answer("With facetting", correct = TRUE),
               answer("It isn't displayed.")
)
```