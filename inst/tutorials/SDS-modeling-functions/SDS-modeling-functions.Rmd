---
title: "Modeling functions"
subtitle: "Stats for Data Science"
author: "D. Kaplan"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(SDSdata)
library(mosaic)
library(mosaicModel)
knitr::opts_chunk$set(echo = FALSE)
```

## Background

We'll use mathematical *functions* as a way of describing relationships among variables. Think of a function as a machine that takes inputs and produces an output. Whenever the  machine is given the same inputs, it will produce  the same output. The functions we will create will produce as output a value  for  the response variable  when given inputs that are values of the explanatory variables.

Modeling functions are similar in spirit to modeling clay. With modeling clay, you can  push, stretch, and prod to make a object with the shape you want. Modeling functions can be shaped however  you  like, but usually we shape  them  to  resemble our  data.  
The means for shaping modeling functions kind of representation you want. Unlike modeling clay, which is  pretty  intuitive to use, for  shaping  modeling  functions we  will  use software. This tutorial introduces that software. Confusingly, the software operations  are also called "functions," so it ends up that  you'll  be  using "functions" (software) to  create and  display "functions" (mathematical). To avoid this confusion, we'll often use the word "model" to refer to  the mathematical objects that we use to  describe relationships.

There are five software functions you will be using  in this  tutorial:

1. `lm()`  which is used  to train unbounded proportional combination models from  data.
2. `glm()` which  is much the same as `lm()`, but produces *bounded* proportional combination models.
3. `mod_plot()` which  makes it easy to graph a model you've created with `lm()` and `glm()`
4. `mod_eval()` which enables  you to specify the inputs and  receive the output from a model.
5. `ns()` which let's you specify to `lm()` and `glm()` how flexible the models they create should be.

## Training models

The work done by `lm()` and `glm()` is usually referred to a "training" a model or -- equivalently -- as "fitting" a model. Training a model is somewhat like training a dog. You approach the training with two sets of ideas: (1) signals or spoken commands that you want to use to elicit an action from the dog, and (2) what those elicited actions should look like -- sit, roll over, fetch, etc.

In training a dog, you repeat some steps over and over. First, give a command. Second, observe whether the dog produces the corresponding action. If so, you give the dog a praise or a treat. If not, you make movements to illustrate the action. You do this over and over and, somehow, in a process little understood by most people,  the animal learns to connect the command  and  the action.

In training a model, you do much the same thing. The commands are the values of the explanatory variables for an individual row from your data. The "action" that you look for is the  value of the response variable for that row. Over and over again you present rows from the data, taking the values of the explanatory variables as the command and insisting that the model produce a value as close as possible to the response variable value for that row.

Just as with training a dog, most people find it hard to understand the internal changes brought about training, many people find mysterious the mathematical processes that produce a function that relates the explanatory variables to the response variables. Dog training seems to require some natural talent and an ability to communicate with dogs. Some people can do it well, some people can't. Fortunately, the "talent" needed to train a model is entirely represented in in the functions `lm()` and `glm()`. You just have to specify which is to be the response variable and what are to be the explanatory variables, and provide a data frame. The rest is carried out automatically by the computer.

## Bounded models

Unbounded models are appropriate when the response variable is *quantitative*. It doesn't matter whether the explanatory variables are quantitative or categorical; the proportional combinations approach is suitable for all kinds of explanatory variables.

Training a unbounded model is accomplished with the `lm()` function  in  R. You need to provide two inputs to `lm()`:

a. a data frame to use for training
b. a model formula specifying the explanatory variable(s) and the response variable.

The output of `lm()` is a trained model. The output is usually given a name so that you can refer to it later, for example to graph it or analyze it further.

To illustrate, let's an unbounded model of the amount of natural gas used for heating a home. The data come from the monthly utility bills for the author's home in Minnesota, USA. The data frame is named `Home_utilities` and comes from the `SDSdata` package. 

The response variable -- the amount of natural gas actually used -- will be `ccf`, which  stands for the volume of gas used in  one month.  (The volume units for natural gas are cubic feet, appreviated ccf.). To start, we'll try to predict `ccf` based on the average temperature, `temp`, during the month.

As you can see in the following code block, the  `lm()` function  is being used. The two arguments to  `lm()` are the model formula and the data  frame. As always, the reponse variable is on the left  side  of the formula. There's only one explanatory variable being used here. It goes on the right side of the formula. The output of `lm()` has been saved under the name `mod1`.

```{r ccf-temp1, exercise = TRUE, exercise.cap = "Train the model"}
mod1 <- lm(ccf ~ temp, data = Home_utilities)
```

Training is very simple; all the work is done by `lm()`. Once you have the trained model, you will want to do something with it. Use the exercise block above to carry out these actions with the model:

1. Plot a graph of the model. Use `mod_plot(mod1)`.
2. Evaluate the model by specifying an input. For instance, to see the predicted natural gas usage when the monthly average temperature is 32°F (corresponding to 0°C) use `mod_eval(mod1, temp = 32)`. 

```{r ccf-temp2, echo = FALSE}
Q1 <- question("Make a graph of the `ccf ~ temp` model. What is the shape of the graph?",
               answer("A constant."),
               answer("A straight line.", correct = TRUE),
               answer("A wavy curve."))
Q2 <- question("Does the output of the model go up or down was the temperature increases? You can judge this from the slope of the graph.", 
               answer("Down. Higher temperature goes with lower `ccf`.", correct = TRUE),
               answer("Up. Higher temperature goes with higher `ccf`.  "),
               answer("Both up and  down."))
Q3 <- question("What is the numerical output of the model when the input is `temp  = 20`?",
               answer("186 cubic feet"),
               answer("187 cubic feet"),
               answer("188 cubic feet", correct = TRUE))
quiz(caption="Looking at `ccf` versus `temp`.", Q1, Q2, Q3)
```

## Not-straight models with `ns()`

Even though the model architecture is called "linear," graphs of the model do not need to be straight lines. In order to have a curvy function,  you will need to say so in the model formula used for training the model. You can do this by giving the explanatory variable as an argument to  the `ns()` function. `ns()` also takes a second argument, called the "degrees of freedom", which can be 1, 2, 3, ... The degrees of freedom describes how much curviness to allow.

```{r ccf-temp5, exercise = TRUE, exercise.cap = "Curvy models with `ns()`."}
mod2 <- lm(ccf ~ ns(temp, 0), data = Home_utilities)
mod_plot(mod2, interval  = "prediction")
```

It's nice to imagine that `ns` means `n`ot `s`traight. It does effectively, but the stickler for mathematical precision will point out that it actually stands for `n`atural `s`pline. A spline is a somewhat stiff but somewhat flexible curve.

```{r ccf-temp6, echo = FALSE}
Q1 <- question("The second argument to `ns()` can be 1, 2, 3,  .... What value for the degrees of freedom corresponds to a straight line.",
               answer("1", correct = TRUE),
               answer("2"), 
               answer("3"))
Q2 <- question("If you make the degrees of freedom large, the model can go up and down somewhat erratically. Which  of these degrees of freedom  produces lots of little ups and downs? ", 
               answer("5"),
               answer("10"),
               answer("20", correct  = TRUE))
quiz(caption="Nonlinear curves", Q1, Q2)
```


## Bounded models

The logistic architecture is appropriate when the response variable is categorical and you want to find the probability that the output will be a particular one of the possible levels. Training a logistic model is done with the `glm()` function. Like `lm()`, you provide a model formula and a data frame as arguments. You must also provide a third argument, which will always be `family = binomial`.

The explanatory variable(s) in a logistic model can be quantitative or categorical

To illustrate, let's model the sex of a person using their height as the explanatory variable. The `Galton` data frame records the `sex` and `height` of about 900 adults measured in the 1890s.

When training a logistic model, you need to write the model formula so that the left-hand side specifies which level you are asking the probability for. This is done by naming the level in quotes and using `==` (double equal signs), which stands for "is the thing on the left equal to the thing on the right?" In `Galton`, the levels for `sex` are `"F"` and `"M"`.

```{r logistic-1, exercise = TRUE, exercise.cap = "Logistic model"}
sex_mod1 <- glm(sex == "F" ~ height, data = Galton, family = binomial)
mod_plot(sex_mod1)
mod_eval(sex_mod1, height = 65)
```

Note that the shape of the logistic model bends as needed to keep the output between 0 and 1, as befits the output's being a probability.

```{r logistic-2, echo = FALSE}
Q1 <- question("When `height = 70` inches, what is the probability of the person being female?",
               answer("zero"),
               answer("About 6%", correct = TRUE),
               answer("About 30%"))
Q2 <- question("When `height = 65` inches, what is the  probability  of the person being male?", 
               answer("About 23%", correct = TRUE),
               answer("About 50%"),
               answer("About 77%", message = "That's the probability of being female. You were asked about the probability of being male."))
Q3 <- question("What happens if you leave out the argument `family = binomial`?",
               answer("R figures out what you meant."),
               answer("R generates an error message."),
               answer("R makes a straight-line model (that doesn't stay in the bounds zero to one).", correct = TRUE))
Q4 <- question("What happens if instead of using two equal signs (`==`) in the formula, you use just one equal sign (`=`)?",
                              answer("R figures out what you meant."),
               answer("R generates an error message.", correct = TRUE),
               answer("R makes a straight-line model (that doesn't stay in the bounds zero to one)."))
quiz(caption = "Logistic regression", Q1, Q2, Q3, Q4)               

```

## Multiple explanatory variables

You can include multiple explanatory variables by adding their names to the model formula with `+`.  Example: Model the sex of a person based on the person's height and also the person's mother's height.

```{r multiple-1, exercise = TRUE, exercise.cap = "Two explanatory variables"}
sex_mod2 <- glm(sex == "F" ~ height + mother, data = Galton, family = binomial)
mod_plot(sex_mod2)
mod_eval(sex_mod2,  height = 65,  mother = 61)
```

```{r multiple-2, echo = FALSE}
Q1 <- question("In the plot of the model made by `mod_plot()`,  how is the second explanatory variable displayed?",
               answer("With color", correct = TRUE),
               answer("With facetting"),
               answer("It isn't displayed."))
Q2 <- question("For a person of height 65 inches, is the person more  likely to be female if the mother's height is 62 or 63 inches?",
               answer("Higher probability of being female when mother is 62 inches."), 
               answer("Higher probability of being female when mother is 63 inches.", correct = TRUE),
               answer("The same probability regardless of whether the mother is 62 or 63 inches tall.", message = "Make sure you are looking at the output of `sex_mod2` rather than `sex_mod1`. Remember that `sex_mod1` doesn't include the mother's height as an explanatory variable."))
Q3 <- question("Include the father's height as an explanatory variable by using the model formula `sex == \"F\" ~ height + mother + father`. In the graph of the model function, how is `father` displayed?",
               answer("With color"),
               answer("With facetting", correct = TRUE),
               answer("It isn't displayed.")
)
```
